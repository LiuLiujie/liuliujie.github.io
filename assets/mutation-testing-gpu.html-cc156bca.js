import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r as i,o as l,c as r,a as e,b as n,e as a,d as s}from"./app-9e8a13db.js";const p={},c=s('<h1 id="testing-and-mutation-testing-on-gpu-kernels" tabindex="-1"><a class="header-anchor" href="#testing-and-mutation-testing-on-gpu-kernels" aria-hidden="true">#</a> Testing and Mutation Testing on GPU kernels</h1><p><strong>Goal</strong>: How existing test theory can be used and adapted to the specific use case of GPU kernels.</p><p>Two directions</p><ul><li>What would be useful <strong>coverage criteria</strong> to estimate the quality of test suites. ✅</li><li>The <strong>generation of test cases</strong> based on either code inspection or user-defined properties.</li></ul><p><strong>My inspiration</strong>:</p><p>Use mutation testing to estimate the quality of test suites.</p><p>Basic of mutation testing: change a bit in the source code and see if the mutants can survive.</p><p><strong>My concerns</strong>:</p>',8),u=e("li",null,[e("p",null,"Is it really feasible/meaningful to introduce mutation testing into GPU programming"),e("ul",null,[e("li",null,[n("From "),e("a",{href:"#blog-testing-gpu-code"},"here"),n(" we may need to figure out if the mutants will pass/fail the test cases too easy. In other words, in which application scenario the mutation testing is useful in GPU programming.")])])],-1),d=e("p",null,"Abstraction level for the mutation testing tool",-1),h=e("li",null,[e("p",null,"Directly written in C/C++: Had better not....")],-1),m=e("p",null,"As a 'plugin' for kernel tuner:",-1),g=e("p",null,"The process is like:",-1),f={href:"https://stryker-mutator.io/docs/mutation-testing-elements/supported-mutators/",target:"_blank",rel:"noopener noreferrer"},k=s("<li><p>A dry run is needed to see if the code can be compiled and started successfully.</p></li><li><p>Run all the mutants and give a report.</p></li><li><p>Pros:</p><ul><li>Can be done under Kernel Tuner&#39;s architecture for dry run and testing framework.</li><li>Higher abstraction level: interact with Kernel Tuner and PyCUDA/PYOpenCL.</li><li>Partially use python&#39;s sophisticated testing tools is possible.</li></ul></li><li><p>Cons:</p><ul><li>Highly depends on the Kernel Tuner, which may limit the development and usage</li></ul></li>",4),b=e("li",null,[e("p",null,[n("An independent tool focus on CUDA/OpenCL mutators. A "),e("a",{href:"#paper-applying-mutation-testing-to-gpu-programs"},"paper"),n(" from TUD.")]),e("ul",null,[e("li",null,[e("p",null,"A whole (mutation) testing framework is much more work (must sharply limit the scope)"),e("ul",null,[e("li",null,"limit the mutators, limit the language. (CUDA/OpenCL)")])]),e("li",null,[e("p",null,"More freedom in implementation.")])])],-1),v=e("li",null,[e("p",null,"How to measure survival/kill"),e("ul",null,[e("li",null,"The output of the program"),e("li",null,[n("The performance of the execution. "),e("ul",null,[e("li",null,"Test cases for performance?"),e("li",null,"The [paper](#[Paper] Applying Mutation Testing to GPU Programs) also find this problem.")])])])],-1),y=s('<h1 id="literature-review" tabindex="-1"><a class="header-anchor" href="#literature-review" aria-hidden="true">#</a> Literature review</h1><h2 id="paper-kernel-tuner-a-search-optimizing-gpu-code-auto-tuner" tabindex="-1"><a class="header-anchor" href="#paper-kernel-tuner-a-search-optimizing-gpu-code-auto-tuner" aria-hidden="true">#</a> [Paper] Kernel Tuner: A search-optimizing GPU code auto-tuner</h2><h3 id="_1-introduction" tabindex="-1"><a class="header-anchor" href="#_1-introduction" aria-hidden="true">#</a> 1. Introduction</h3><p>In brief: An auto tuner for tuning GPU programming parameters</p><p>Problems:</p><ul><li>Different parameters for OpenCL/CUDA kernel yield different performance on a same application</li><li>These parameter values are application specific</li><li>An auto tuner is needed to find the</li></ul><p>Feature of the tool:</p><ul><li>Find optimized parameters for</li><li>TDD</li></ul><p>Methods:</p><ul><li>Random</li><li>Global optimization Algorithm</li><li>Several strategies support</li></ul><h3 id="_2-related-work" tabindex="-1"><a class="header-anchor" href="#_2-related-work" aria-hidden="true">#</a> 2. Related work</h3><p>Two main research directions:</p><ul><li>this research focus on user-defined code parameterization</li></ul><p>Two techniques</p><ul><li>Performance model <ul><li>ML based</li><li>static and predictive analysis</li><li>Cons: no performance guarantees for general usages</li></ul></li><li>Empirical performance measurement <ul><li>Cons: <ul><li>tunning process itself can be time-consuming and input specific</li><li>random search sometimes is more effective than algorithms</li></ul></li></ul></li></ul><p>What the author do based on above:</p><ul><li>A mashup of algorithms support</li></ul><h3 id="_3-design-and-implementation" tabindex="-1"><a class="header-anchor" href="#_3-design-and-implementation" aria-hidden="true">#</a> 3. Design and implementation</h3><h4 id="_3-1-user-interface" tabindex="-1"><a class="header-anchor" href="#_3-1-user-interface" aria-hidden="true">#</a> 3.1 User interface</h4><p>Two important function</p><ul><li>run_kernel: run a specific kernel config</li><li>tune_kernel: user defines init values for parameters</li></ul><p>One important arguments</p><ul><li>Kernel_name: code in string or in a file</li></ul><h4 id="_3-2-strategies" tabindex="-1"><a class="header-anchor" href="#_3-2-strategies" aria-hidden="true">#</a> 3.2 Strategies</h4><ol><li>Strategies <ul><li>Random sample</li><li>Minimize</li><li>Basin hopping (BH)</li><li>...</li></ul></li><li>Problem: Algorithm continuous space -&gt; Configuration discrete space <ol><li>Snap variable to nearest kernel configuration, check user-defined restrictions</li><li>(except DE, GA and SA strategies) Normalize to [0, 1]</li><li>Cache measured execution time of evaluated configurations. Avoid measuring again for the one &#39;mapping&#39; to the same configuration.</li></ol></li><li>Strategy parameters: use a default one, allowing user changes</li></ol><h4 id="_3-3-runners-and-backends" tabindex="-1"><a class="header-anchor" href="#_3-3-runners-and-backends" aria-hidden="true">#</a> 3.3 Runners and backends</h4><p>Sequential runner &amp; Noodles parallel runner</p><p>PyCUDA &amp; PyOpenCL: An interface to access CUDA/OpenCL APIs</p><h3 id="_4-tdd-support" tabindex="-1"><a class="header-anchor" href="#_4-tdd-support" aria-hidden="true">#</a> 4 TDD support</h3><p>User&#39;s jobs</p><ul><li>Write the CUDA/OpenCL code</li><li>Write the test cases in Python</li><li>Trigger run_kernel to execute the test cases</li></ul><h2 id="blog-testing-gpu-code" tabindex="-1"><a class="header-anchor" href="#blog-testing-gpu-code" aria-hidden="true">#</a> [Blog]Testing GPU code</h2><p>Steps without framework:</p><ol><li><p>Decide on some input data</p></li><li><p>Compile the GPU code (if not compiled already)</p></li><li><p>Allocate GPU memory</p></li><li><p>Copy input data to the GPU</p></li><li><p>Setup thread block and grid dimensions</p></li><li><p>Call the kernel</p></li><li><p>Copy data back to host memory</p></li><li><p>Free GPU memory</p></li><li><p>Check the behavior of the system under test</p></li></ol><p>Kernel Tuner can do 2-8 steps.</p>',35),_={href:"https://blog.esciencecenter.nl/writing-testable-gpu-code-23bbda3a5d62",target:"_blank",rel:"noopener noreferrer"},w={href:"https://stackoverflow.com/questions/12373940/difference-between-global-and-device-functions",target:"_blank",rel:"noopener noreferrer"},x=s('<h3 id="design-test-case" tabindex="-1"><a class="header-anchor" href="#design-test-case" aria-hidden="true">#</a> Design test case</h3><p>The range of input data may influence the output. We really need to consider whether it will make the test useless when mutation happens.</p><ul><li>All mutants will easily fail the test, but there might still a lot of bugs in the code.</li><li>All mutants will pass the test, it may because the output variance is not sufficient.</li></ul><h2 id="paper-applying-mutation-testing-to-gpu-programs" tabindex="-1"><a class="header-anchor" href="#paper-applying-mutation-testing-to-gpu-programs" aria-hidden="true">#</a> [Paper] Applying Mutation Testing to GPU Programs</h2><h3 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h3><table><thead><tr><th>Comparation</th><th>CPU</th><th>GPU</th></tr></thead><tbody><tr><td>Computing</td><td>Focus on correctness and efficiency of singel thread, leave the thread collaboration to libraries</td><td>Manage the threads explicitly</td></tr><tr><td>Memory access</td><td>Transparant to programmers</td><td>Explicitly manage different type of memory (shared mem, global mem, host mem)</td></tr></tbody></table><h3 id="contribution" tabindex="-1"><a class="header-anchor" href="#contribution" aria-hidden="true">#</a> Contribution</h3><ul><li><p>9 new GPU-specfic mutation operators and 5 conventional mutation operators</p></li><li><p>MUTGPU</p><ul><li><p>Architecture of MUTGPU from the paper</p><img src="https://objectstorage.eu-frankfurt-1.oraclecloud.com/n/fripr8xktyle/b/bucket1/o/1669968695500.png" alt="Architecture of MUTGPU" style="zoom:67%;"></li></ul></li></ul><h3 id="gpu-specifc-mutation-operators" tabindex="-1"><a class="header-anchor" href="#gpu-specifc-mutation-operators" aria-hidden="true">#</a> GPU-Specifc Mutation Operators</h3><p>Three directions, nine mutation operators</p><p>Memory Management</p>',11),C=s(`<li><p>Execution configuration</p><ul><li><p>Operators</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code>origin<span class="token operator">:</span> 					add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">4096</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">;</span>
alloc_swap<span class="token operator">:</span>				add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">4096</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">;</span>
alloc_increment<span class="token operator">:</span>	add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">4096</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">;</span>
alloc_decrement<span class="token operator">:</span>	add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">4096</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li></li></ul></li>`,1),U=e("p",null,"Shared Memory",-1),T={href:"https://stackoverflow.com/questions/3841877/what-is-a-bank-conflict-doing-cuda-opencl-programming",target:"_blank",rel:"noopener noreferrer"},P=s(`<ul><li><p>Operators</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code>origin<span class="token operator">:</span>					__shared__ <span class="token keyword">float</span> cache<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
share_removal<span class="token operator">:</span>	<span class="token keyword">float</span> cache<span class="token punctuation">[</span>N<span class="token punctuation">]</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li></li></ul>`,1),I=s(`<p>Thread Management</p><ul><li><p>GPU indexing</p><p>​ GPU programming introduce a new indexing mechanism to iterate the data and thread, using built-in variables such as <code>tgreadId.x</code> and <code>blockIdx.x</code>.</p><ul><li><p>Operators</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code>origin<span class="token operator">:</span>									<span class="token keyword">int</span> tid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
gpu_index_replacement<span class="token operator">:</span>	<span class="token keyword">int</span> tid <span class="token operator">=</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
gpu_index_increment<span class="token operator">:</span> 		<span class="token keyword">int</span> tid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
gpu_index_decrement<span class="token operator">:</span>		<span class="token keyword">int</span> tid <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>Discussion</p><ul><li>qute a lot of mutants in their results</li></ul></li></ul></li><li><p>Synchronisation functions</p><p>​ Used to coordinate communications between threads in a specfic block</p><ul><li><p>Operators</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code>origin<span class="token operator">:</span>					<span class="token function">__syncthreads</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
sync_removal<span class="token operator">:</span>		<span class="token comment">//__syncthreads();</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li></li></ul></li></ul><p>Atomic operations</p><p>​ Guranteed to be performed without interference from other threads.</p><ul><li><p>Operators</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code>origin<span class="token operator">:</span>					<span class="token function">atomicAdd</span><span class="token punctuation">(</span><span class="token operator">&amp;</span><span class="token punctuation">(</span>histo<span class="token punctuation">[</span>buffer<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span>
atom_removal<span class="token operator">:</span>		histo<span class="token punctuation">[</span>buffer<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>		
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>Discussion</p><ul><li>Only one mutant: subject systems do not contains much atomic operations</li></ul></li></ul><h3 id="problems-they-find-limitations" tabindex="-1"><a class="header-anchor" href="#problems-they-find-limitations" aria-hidden="true">#</a> Problems they find/ Limitations</h3><ol><li>Most tests from the six projects do not target unit-level: invoke a series of functions and examine the final results <ul><li>Their solution: add more test cases</li></ul></li><li>A samll change in the program (a mutant) do not propagete to the result. <ul><li>Their idea: split the functions or refactor to non-void, but it is too complicate so thay just leave aside.</li></ul></li><li>Many functions cannot be accessed from the test suites. <ul><li>Test cases are located at <strong>host</strong> level, which invoke functions in <strong>device/host</strong> and examine the results. <ul><li>Workaround: wrap <code>__device__</code> functions with <code>__global__</code> function.</li></ul></li><li>Static and inline functions <ul><li>remove static and inline</li></ul></li></ul></li><li>Mutants not affecting the result of the kernel functions but the GPU performance.</li></ol><h3 id="my-thinking" tabindex="-1"><a class="header-anchor" href="#my-thinking" aria-hidden="true">#</a> My thinking</h3><h4 id="_1-test-driven-development" tabindex="-1"><a class="header-anchor" href="#_1-test-driven-development" aria-hidden="true">#</a> 1. Test-driven development:</h4><p>​ I can image that quite a lot of GPU project is not under the guideline of test-driven development. This is because unlike CPU programming, which is bussiness-oriented and having a clear workflow, the GPGPU programming is much research-oriented and testing for each unit is quite costly. They can just redesign and run their code again and again, or they even don&#39;t care/know what&#39;s the expected output of a single function.</p><p>​ To introudce TDD/ mutation testing into this area, an out-of-box and easy way to perform testing is needed. From the paper we know that writing a test at host level to kill a mutant is much easy, but writing a test for a device function is much harder and need further investigation on the context of the code. So I think there are two directions to :</p><ol><li>An out-of-box way to design and implement a test case for GPU functions? <ul><li>A testing framework for CUDA/OpenCL is necessary?</li></ul></li><li>A test cases generation tool for GPU fucntions?</li></ol><h4 id="_2-mutation-testing-or-a-lint" tabindex="-1"><a class="header-anchor" href="#_2-mutation-testing-or-a-lint" aria-hidden="true">#</a> 2. Mutation testing or a lint?</h4><p>​ Some running examples in the paper that be detected by mutation testing may also be detected by a lint. So is it necessary to use mutation testing for these defect? This is because comparing to the statistic analysis, the mutation testing is definetly costly.</p>`,14),A=s(`<p>alloc_increment, alloc_decrement</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code>alloc_increment<span class="token operator">:</span>	add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">4096</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">;</span>
alloc_decrement<span class="token operator">:</span>	add<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">4096</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">,</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div>`,2),G={href:"https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/",target:"_blank",rel:"noopener noreferrer"},D=s(`<li><p>some synchronisation functions</p><p>List 18 of the paper: &quot;The code fragment in Listing 18 does not contain write operations to the shared array <em>sum</em> and <em>sum2</em>, thereby, there is no need for a synchronisation function (<em>cg::sync(cta)</em>) in the end to guarantee that all of those writes to the shared arrays complete before anyone tries to read from the buffers.&quot;</p><div class="language-c line-numbers-mode" data-ext="c"><pre class="language-c"><code><span class="token keyword">if</span> <span class="token punctuation">(</span>tid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">{</span>
    beta  <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>beta2 <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> blockDim<span class="token punctuation">.</span>x<span class="token punctuation">;</span> i <span class="token operator">+=</span> VEC<span class="token punctuation">)</span> <span class="token punctuation">{</span>
        beta  <span class="token operator">+=</span> sum<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span>beta2 <span class="token operator">+=</span> sum2<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">}</span>
    __TOptionValue t  <span class="token operator">=</span> <span class="token punctuation">{</span>beta<span class="token punctuation">,</span> beta2<span class="token punctuation">}</span><span class="token punctuation">;</span>
    <span class="token operator">*</span>d_CallValue <span class="token operator">=</span> t<span class="token punctuation">;</span><span class="token punctuation">}</span>
cg<span class="token operator">::</span><span class="token function">sync</span><span class="token punctuation">(</span>cta<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Can a lint find this defect through statistic code analysis?</p></li>`,1),M=e("p",null,"​ Searching for lint patterns and see if it can be used as mutation operators may also be useful.",-1),S=e("h4",{id:"_3-performance-testing",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_3-performance-testing","aria-hidden":"true"},"#"),n(" 3. Performance testing")],-1),q=e("p",null,'In the paper they discard the mutants that influence the performance, but they also offer another idea that "using performance requirement to be part of the definition of a test case passing or failing".',-1),L=e("p",null,[e("strong",null,"One more thing"),n(": a small bug in the paper? 😛")],-1),O=e("p",null,"In listing 2 line 13 they try to pass an integer array to a float type array parameters, and my compiler doesn't feel happy about this when I try to replicate this demo.",-1),R=e("h2",{id:"paper-cltestcheck-measuring-test-effectiveness-for-gpu-kernels",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#paper-cltestcheck-measuring-test-effectiveness-for-gpu-kernels","aria-hidden":"true"},"#"),n(" [Paper] CLTestCheck: Measuring Test Effectiveness for GPU Kernels")],-1),N={href:"https://github.com/chao-peng/CLTestCheck",target:"_blank",rel:"noopener noreferrer"},W={href:"https://github.com/chao-peng/CLTestCheck/blob/master/clmt/src/Utils.cpp#L166",target:"_blank",rel:"noopener noreferrer"},E=s('<h3 id="how-this-tool-works" tabindex="-1"><a class="header-anchor" href="#how-this-tool-works" aria-hidden="true">#</a> How this tool works?</h3><ul><li>Based on a tool Clang LibTooling, a parser for C/C++</li><li>It rewrites the source code with the mutation operators.</li><li>Then it compiles and execute the mutants.</li><li>A reference result file needs to be provided to compare it with the output. (Only when the output file and the reference file are exactly the same)</li></ul><h3 id="my-thinking-1" tabindex="-1"><a class="header-anchor" href="#my-thinking-1" aria-hidden="true">#</a> My thinking</h3><ol><li>Something we need to consider in our research <ol><li>Kernel code coverage <ul><li>Line coverage</li><li>Branch coverage: executed by at least one thread&quot;.</li><li>Loop Boundary Coverage</li><li>Barrier coverage:</li></ul></li><li>Hit counter and timeout mechanism <ul><li>Some mutants may timeout when they go to invalid loop or deadlock</li><li><code>#pragma unroll</code> might be influenced in mutation testing? (need further investigate)</li></ul></li></ol></li></ol><h1 id="week-1-retrospective" tabindex="-1"><a class="header-anchor" href="#week-1-retrospective" aria-hidden="true">#</a> Week 1 Retrospective</h1><h2 id="what-i-did-this-week" tabindex="-1"><a class="header-anchor" href="#what-i-did-this-week" aria-hidden="true">#</a> What I did this week</h2><ol><li>Configure an environment for CUDA/OpenCL programming based on Jupyter Lab with both NVIDIA/AMD GPUs. Test it with some demos. (Sadly Apple doesn&#39;t support OpenCL anymore on arm64)</li><li>Read the paper &quot;Applying Mutation Testing to GPU Programs&quot; carefully. Go <a href="#paper-applying-mutation-testing-to-gpu-programs">here</a>.</li><li>Write a draft for my study plan.</li></ol><h2 id="research-plan" tabindex="-1"><a class="header-anchor" href="#research-plan" aria-hidden="true">#</a> Research Plan</h2><p>After reading the paper, I soon notices that the learning curve in GPGPU is quite steep. To tackle the problem I firstly need a researchplan to define 1) What I need to study 2)How deep I need to go? 3) Time schedule.</p><h3 id="scope-of-this-research" tabindex="-1"><a class="header-anchor" href="#scope-of-this-research" aria-hidden="true">#</a> Scope of this research</h3><ol><li>Limit to CUDA, C/C++, Python first. Then extend to OpenCL if time permitted.</li></ol><h3 id="plan" tabindex="-1"><a class="header-anchor" href="#plan" aria-hidden="true">#</a> Plan</h3><p>Three directions to go. Concrete schedule is not decided yet but it should be done before Jan 20th so that I can have at least 2 weeks for my proposal.</p>',13),K=e("p",null,"CUDA programming",-1),j=e("li",null,[n("Goal: "),e("ul",null,[e("li",null,"Understand what the typical CUDA programs are going to do."),e("li",null,"Can write test cases for CUDA programs")])],-1),z=e("li",null,"[x] [Done] Config the experiment environment for it.",-1),B=e("li",null,'[x] [Started at Nov. 26th] Basic idea of the architecture of CPU/GPU model and CUDA: read the book "The CUDA Handbook"',-1),H={href:"https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/",target:"_blank",rel:"noopener noreferrer"},F={href:"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html",target:"_blank",rel:"noopener noreferrer"},V=e("li",null,"[ ] Research on testing for CUDA programs",-1),Q=e("p",null,"Kernel Tuner",-1),J=e("ol",null,[e("li",null,[n("Goal: "),e("ul",null,[e("li",null,"Understand how this tool work (no need to dive into concrete patterns and optmization algorithms, but how the tuner process work)"),e("li",null,"Can write a module/component for the tool")])]),e("li",null,"Todos:")],-1),Z=e("li",null,"[x] Trying the basic demo in kernel Tuner GitHub projects",-1),Y={href:"https://github.com/KernelTuner/kernel_tuner/tree/master/examples",target:"_blank",rel:"noopener noreferrer"},X=e("li",null,[n("[ ] Source code reading, focus on "),e("ul",null,[e("li",null,"[x] Tuner: especially how the parameters is changed and how the performance of kernel is measured."),e("li",null,"[ ] Testing interface"),e("li",null,[n("[ ] Figure out if it is possible to support/going to support: "),e("ul",null,[e("li",null,"[ ] Test device function"),e("li",null,"[ ] Design and execute test cases about performance.")])])])],-1),$=s("<li><p>Testing/ Mutation testing</p><ul><li>[x] Try to search and read papers in this area related mutation testing and GPU <ul><li><p>[x] GPU related: fault injection GPU, CUDA testing framework, test and coverage measurement GPU</p></li><li><p>[x] Mutation operators: C/C++ mutation operators</p></li><li><p>[x] Performance: How to write a test case to measure performance</p></li><li><p>[x] Run the <code>replicate_package</code> of the paper</p></li></ul></li></ul></li>",1),ee=s('<h1 id="week-2-retrospective" tabindex="-1"><a class="header-anchor" href="#week-2-retrospective" aria-hidden="true">#</a> Week 2 Retrospective</h1><h2 id="what-i-did-this-one-and-a-half-week" tabindex="-1"><a class="header-anchor" href="#what-i-did-this-one-and-a-half-week" aria-hidden="true">#</a> What I did this one and a half week</h2><ol><li>Following the above study plan <ul><li>Read, try to understand and reproduce most of the examples in Kernel Tuner&#39;s tutorial.</li><li>Read some CUDA tutorials and books, reporduce some demos.</li><li>Search on mutation testing, CUDA testing framework, test and coverage measurement in GPU and performance</li></ul></li><li>A paper CLTestCheck: Measuring Test Effectiveness for GPU Kernels</li></ol><h2 id="things-to-discuss" tabindex="-1"><a class="header-anchor" href="#things-to-discuss" aria-hidden="true">#</a> Things to discuss</h2>',4),ne=e("p",null,"How to test CUDA code?",-1),te={href:"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#reference-comparison",target:"_blank",rel:"noopener noreferrer"},ae=s("<ul><li>Reference Comparison (at least Integration level)</li><li>Unit test: recommended to execute on CPU instead of GPU <ul><li>Write our CUDA kernels as a collection of many short <code>__device__</code> functions rather than one large monolithic <code>__global__</code> function and test before hooking them all together.</li><li>Defined functions as <code>__host__ __device__</code> rather than just <code>__device__</code> functions, then these functions can be tested on both the CPU and the GPU.</li></ul></li></ul>",1),se={href:"https://kerneltuner.github.io/kernel_tuner/stable/correctness.html",target:"_blank",rel:"noopener noreferrer"},oe=s("<li><p>Unit test using features of Kernel Tuner</p><ul><li>Necessary: Mutation testing is mostly used at unit test level.</li><li>I noticed that all CUDA demos I met are only containing <strong>one device/global function</strong>, so does the examples in the Kernel Tuner&#39;s tutorial. So I have a question about this: In medium or large size project, which may contains <strong>many device/global</strong> functions that need to be parameter-optimized, how we optimize them? Or precisely, what&#39;s the typical scenario of using Kernel Tuner in these kind of projects? Should we tune per function, or only use if for the entry function (main function) and it will do some magic to optimize the entire project?</li><li>【Direction 1】A module built in Kernel Tuner, and the user only need to call something like <code>kernel_tuner.mut_kernel</code> once. After that this module will generate the mutants from the kernel and execute them and compare the result with user inputs from the parameters <code>answer=reference, verify=verify_partial_reduce</code> that Kernel Tuner has already support. <ul><li>Testing level depends on the level of the kernel. <ul><li>If the kernel is a single function, it will be a unit test.</li><li>If the kernel is a complicated function calling a lot of other functions, it may be a integration test.</li></ul></li></ul></li></ul></li><li><p>Unit test using CPU: C/C++ testing framework available/compatible?</p><p>Since CUDA can work with C/C++, is it possible to test it in a C/C++ compatible way?</p><ul><li><p>E.g. GoogleTest: seems mostly compatible with CUDA, some branch situations need to further investigate.</p></li><li><p>Test cases can/need be written in C/C++. The users have much more freedom to choose which functions they want to test.</p></li><li><p>【Direction 2】Modify the Kernel Tuner to make it easy to execute C/C++ test cases that are written in C/C++ in the source code of kernel, with some help of C/C++ testing framework; It will be nice if this framework also support mutation testing. If it is not the case, another C/C++ mutation testing framework may also be included.</p></li></ul></li><li><p>Conclusion:</p><ol><li><strong>【Direction 1】</strong> is at a higher level, using the functionality of Kernel Tuner and the test cases can/need to be written in Python (to verify the result that the Kernel Tuner returns). <ul><li>Personally prefer this one since it is closer to the topic of this research.</li></ul></li><li><strong>【Direction 2】</strong> is at a lower level, giving more freedom to the user since they can define the test cases in their own style, the Kernel Tuner is only responsible for executing these test cases for (mutation) testing. <ul><li>Much more ambitious. Need to precisely decide where to put your additions</li></ul></li></ol></li>",3),ie=e("p",null,"Mutation operators for CUDA/OpenCL",-1),le=e("p",null,"We will refer and conclude the operators we are going to use in this research from the following sources. Open to new operators during the process.",-1),re={href:"https://link.springer.com/chapter/10.1007/978-3-030-16722-6_19",target:"_blank",rel:"noopener noreferrer"},pe={href:"https://pure.tudelft.nl/ws/portalfiles/portal/124438843/09159103.pdf",target:"_blank",rel:"noopener noreferrer"},ce={href:"https://stryker-mutator.io/docs/stryker-net/mutations/",target:"_blank",rel:"noopener noreferrer"},ue=e("li",null,[e("p",null,"Coverage:"),e("ul",null,[e("li",null,[e("p",null,'Coverage is important in this research since it is necessary when doing result evaluation. e.g. "The test suites make it 95% xxx coverage but only 30% mutation score".')]),e("li",null,[e("p",null,"Need to consider how to introduce 'coverage' to kernel using Kernel Tuner.")]),e("li",null,[e("p",null,[n("I found some from "),e("a",{href:"#paper-cltestcheck-measuring-test-effectiveness-for-gpu-kernels"},"the paper"),n(", some coverage idicators need further investigate.")])])])],-1),de=e("p",null,"Performance measurement:",-1),he={href:"https://kerneltuner.github.io/kernel_tuner/stable/metrics.html",target:"_blank",rel:"noopener noreferrer"},me=e("li",null,'Examples are using execution time or metrics "GFLOP/s", but this may very from GPU devices.',-1),ge=e("li",null,`Haven't found a good way to "test performance", but comparing the performance with the previous executions or letting the user set a baseline for performance can be an idea.`,-1),fe=s(`<h1 id="week-3-retrospective" tabindex="-1"><a class="header-anchor" href="#week-3-retrospective" aria-hidden="true">#</a> Week 3 Retrospective</h1><p>This week I mainly focus on literature review. I also have started writing the proposal.</p><h2 id="what-zhu-et-al-do" tabindex="-1"><a class="header-anchor" href="#what-zhu-et-al-do" aria-hidden="true">#</a> What Zhu et al. do</h2><p>The source code is included in the replicate_package.</p><p>They create a dictionary include all the mutation operators and scan the code in CUDA <code>.cu</code> file. If the tool find the key of the dict in the source code, they replace the key with the value.</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>conditional_boundary_replacement <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;&lt;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&lt;=&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&lt;=&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&lt;&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&gt;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&gt;=&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&gt;=&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&gt;&#39;</span><span class="token punctuation">}</span>
negate_conditional_replacement <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;==&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;!=&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;!=&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;==&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&lt;=&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&gt;&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&gt;=&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&lt;&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&lt;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&gt;=&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&gt;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&lt;=&#39;</span><span class="token punctuation">}</span>
math_replacement <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;+&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;-&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;-&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;+&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;*&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;/&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;/&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;*&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;%&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;*&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&amp;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;|&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;|&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&amp;&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;^&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&amp;&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&lt;&lt;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&gt;&gt;&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;&gt;&gt;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&lt;&lt;&#39;</span><span class="token punctuation">}</span>
increment_replacement <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;++&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;--&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;--&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;++&#39;</span><span class="token punctuation">}</span>
logical_replacement <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;&amp;&amp;&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;||&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;||&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&amp;&amp;&#39;</span><span class="token punctuation">}</span>
gpu_index_replacement <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;threadIdx.x&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;blockIdx.x&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;blockIdx.x&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;threadIdx.x&#39;</span><span class="token punctuation">,</span>\\
                         <span class="token string">&#39;threadIdx.y&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;blockIdx.y&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;blockIdx.y&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;threadIdx.y&#39;</span><span class="token punctuation">,</span>\\
                         <span class="token string">&#39;threadIdx.z&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;blockIdx.z&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;blockIdx.z&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;threadIdx.&#39;</span><span class="token punctuation">}</span>
gpu_index_replacement2 <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;threadIdx.x&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;threadIdx.y&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;threadIdx.y&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;threadIdx.x&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;blockIdx.x&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;blockIdx.y&#39;</span><span class="token punctuation">,</span>\\
                           <span class="token string">&#39;blockIdx.y&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;blockIdx.x&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;gridDim.x&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;gridDim.y&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;gridDim.y&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;gridDim.x&#39;</span><span class="token punctuation">}</span>
share_removal <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;__shared__&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;&#39;</span><span class="token punctuation">}</span>
sync_removal <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;__syncthreads()&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;//__syncthreads()&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;cg::sync(cta)&#39;</span><span class="token punctuation">:</span><span class="token string">&#39;//cg::sync(cta)&#39;</span><span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="research-goals-questions" tabindex="-1"><a class="header-anchor" href="#research-goals-questions" aria-hidden="true">#</a> Research goals &amp; questions</h2><p>I tried different ways but can only found two paper related to this area, one for CUDA and one for OpenCL. (We have discussed them in the last two update.) They are both empirical studies, developing correspoind tools and use them in some benchmarking projects. This means they implicitly assume that mutation testing can be theoretically applied in GPU programming, then use their tools to prove it. So in this reseach, besides tool development, I think doing some theoretical research on how existing mutation testing theories can be applied to GPU can also be meaningful.</p><h5 id="based-on-this-i-made-the-following-questions-and-have-a-literature-review-on-mutation-testing" tabindex="-1"><a class="header-anchor" href="#based-on-this-i-made-the-following-questions-and-have-a-literature-review-on-mutation-testing" aria-hidden="true">#</a> Based on this, I made the following questions and have a literature review on mutation testing.</h5><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>\\item \\textbf{G1}: A comprehensive theoretical research on how existing mutation testing theories and techniques can be used in GPU programming, and what can be the new bedrock and inspirations in this area.

\\item \\textbf{G2}: Create an open-source mutation testing tool for GPU kernel development, performing an empirical study to evaluate the conclusions in goal \\textbf{G1} and for kernel developer&#39;s usage.
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><pre><code>\\item \\textbf{RQ1}: What and how existing mutation testing theories can be applied in GPU programming? 
(sub-questions)
\\item \\textbf{RQ2}: What can be new mutation testing theories and techniques specific to GPU kernel?
\\item \\textbf{RQ3}: How to develop the mutation testing tool?
\\item \\textbf{RQ4}: How to use the tool developed in answering \\textbf{RQ3} to evaluate the conclusions we made in the research of \\textbf{RQ1} and \\textbf{RQ2} if necessary.
</code></pre><h2 id="mutation-testing-literature-review" tabindex="-1"><a class="header-anchor" href="#mutation-testing-literature-review" aria-hidden="true">#</a> Mutation testing literature review</h2><p>I found a comprehensive review and some papers regarding the studies about mutation testing.</p><h3 id="theory-of-mutation-testing" tabindex="-1"><a class="header-anchor" href="#theory-of-mutation-testing" aria-hidden="true">#</a> Theory of mutation testing</h3><p><strong>Two fundamental hypotheses</strong></p><ul><li>Competent Programmer Hypothesis (CPH) <ul><li>Programmers tend to develop programs close to the correct version, so we assume that these faults are merely a few simple faults which can be corrected by a few small syntactical changes.</li></ul></li><li>Coupling Effect <ul><li>Simple test cases that can distinguish one sime error can also distinguish more complex error.</li></ul></li></ul><p>Quite a lot of papers are written to prove these hypotheses, in both empirical and theorical ways.</p><p><strong>The Problems of Mutation Analysis</strong></p><ul><li>Cost optimization</li><li>Equivalant mutant</li></ul><p><strong>Cost optimization techniques</strong></p><ul><li>Mutant reduction <ul><li>Reduce the number of mutant in some ways without (dramatically) influence the mutation score.</li><li>Methods <ul><li>Mutant Sampling: randomly or use some algotrithm</li><li>Mutant Clustering: (I haven&#39;t figure it out yet 😛 )</li><li><strong>Selective Mutation</strong>: some mutation operators are more important, omit the less importnat ones</li></ul></li><li>We can also conduct some methods above, like Mutant Sampling, Selective Mutation can be intuitively easier to evaluate.</li></ul></li><li>Execution Cost Reduction <ul><li>Strong, weak and firm mutation <ul><li>Strong Mutation: traditional one, check the output of the function/module</li><li>Weak Mutation: check immediately after the mutant execution.</li><li>Non-deterministic behaviors may strongly influence the effectiveness of week mutation</li><li>Firm mutation: provide a continuum of intermediate possibilities. (No tools has implemented this method)</li></ul></li><li>Runtime Optimization <ul><li>Interpreter-Based: mutate the code in the interpreter level, not suitable in this reasearch</li><li>Compiler-Based: mutate the code in the compiler level, not suitable in this research</li><li>Bytecode Translation Technique: mutate the code in the compiled object</li></ul></li></ul></li><li>Run the mutants in parallel <ul><li>I am currently doing a similar project, running all the mutants in parallel in the cloud.</li><li>Intuitively possible, since GPU is good at parallel computing. <ul><li>Currently we need to get the result of one mutant exection back to CPU and then run the next mutant.</li><li>In this scenerio, one bottleneck of GPU program is GPU-CPU commucation.</li><li>Trasfer all the mutants to the GPU in some ways, and execute only once in parallel and get all the results once.</li></ul></li><li>Maybe possible. Do the check in GPU, a python lib offer a way to check this.</li><li>Not that easy, maybe left it for another master project or my possible Phd.</li></ul></li></ul><p><strong>Equivalant mutant detection</strong></p><ul><li>Equivalant mutants refer to the mutants that can never be killed because they always produce the same output as the original program.</li><li>In research of Zhu el al., they manually discover the equivalant mutants.</li><li>We will have face this problem in our research bacause the mutants have the same output but have different execution time or resource usage are equivalant mutant, and GPU programming is good at producing this kind of results.</li><li>Thanksfully, Kernel Tuner seems good at dealing with this kind of problem and I will try to figure out a solution to deal with it.</li></ul><h2 id="what-should-include-in-this-research" tabindex="-1"><a class="header-anchor" href="#what-should-include-in-this-research" aria-hidden="true">#</a> What should include in this research?</h2><p>At least at this peraration stage, I plan to include the following study into this research.</p><ul><li>[ ] Proof of fundamental hypotheses. I intuitively guess the theoretical proof won&#39;t be that different between CPU and GPU programming, so the empirical proof may be more persuasive.</li><li>[ ] Cost optimization: <ul><li>[ ] Selective Mutation. Benchmarking the mutation operators we used and figure out the &#39;core operator list&#39; and &#39;full operator list&#39;, offer it as a function of our mutation testing tool if possible.</li><li>[ ] Run the mutants in parallel</li></ul></li><li>[ ] Equivalant mutant detection: We try to kill the mutants that have the same output but have different execution time or resource usage.</li></ul><h2 id="next-sprint" tabindex="-1"><a class="header-anchor" href="#next-sprint" aria-hidden="true">#</a> Next sprint</h2><ul><li>[x] Continue to read the papers mentioned in above literature review.</li></ul><p>Goal of reading: figure out if and how their prove/methodology/techqiue can be (partially) migrated/used in GPU programming.</p><ul><li>[x] Focusing on the studies mentioned above</li><li>[x] Search and read papers about quality assurance techniques that used in GPU programming.</li><li>[x] Continue writing the proposal.</li><li>[x] Plan for: Collect the tests/benchmarking projects.</li><li>[x] Code Coverage</li></ul><h1 id="week-4-retrospective" tabindex="-1"><a class="header-anchor" href="#week-4-retrospective" aria-hidden="true">#</a> Week 4 Retrospective</h1><h2 id="code-coverage" tabindex="-1"><a class="header-anchor" href="#code-coverage" aria-hidden="true">#</a> Code coverage</h2><p>I find some papers about verifier/testing tools for GPU kernel but I don&#39;t have enough knowledge-base to dig into them. So at the first step, I mainly focus on how they measure the code coverage in these papers.</p><p>GKLEE is the only tool that provides support for code coverage for CUDA GPU kernels.（from a paper first pub in 2019）</p><p>The coverage concepts we can use in GPU</p><ul><li>Line/statement coverage: almost the same but has some difference, details below</li><li>Branch coverage</li><li>Loop Boundary Coverage <ul><li>Calculate the following cases seperately (sum up to 100%): <ul><li>No loop: <ul><li>E.g: There is a for loop but the execution never goes into it</li><li>num of no loop loops / totol loops</li></ul></li><li>Exactly one loop</li><li>More than one loop</li><li>Boundary value is reached</li></ul></li></ul></li><li>Barrier Coverage <strong>GPU-verify</strong><ul><li>Barrier is a synchronisation mechanism for threads within a thread block/work-group and is used to prevent intra block/work-group data race errors.</li><li>Barrier divergence occurs when the number of threads within a work-group executing a barrier is not the same as the total number of threads in that work-group.</li><li>Covered barrier: a correct execution of a barrier without barrier divergence</li><li>Coverage = covered barriers / total barriers</li></ul></li></ul><h2 id="coverage-in-multi-thread" tabindex="-1"><a class="header-anchor" href="#coverage-in-multi-thread" aria-hidden="true">#</a> Coverage in multi-thread</h2><p>However, some papers concern about &quot;Given that a kernel is usually executed by a large number of threads, there is a real danger, especially with complex/large kernels, <strong>that multiple threads may end up covering some line/branch while no threads visit other lines/branches&quot;</strong></p><p>Their solution: &quot;we keep track of whether some feature (line or branch) is covered by all the threads at least once, or some thread at least once.&quot;</p><p>So intuitively we can have three different kind of &quot;coverage&quot;:</p><ul><li>Avg. Covt: measures the number of lines/statements covered by threads across the whole program, averaged over the threads</li><li>Min. Covt: measures the minimum by any thread</li><li>Max. Covt: measures the maximum by any thread</li></ul><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code># We have 3 thread
if (thread.id &lt;= 1){
	# 3 lines of code
	# 2 threads go here
}else{
	# 27 lines of code
	# 1 threads go here
}

30 lines in total
Avg. Covt: (2 * 3/30 + 1 * 27/30) / 3 = 36.7%
Min. Covt: 3/30 = 10%
Max. Covt: 27/30 = 90%
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="fundamental-hypotheses-coupling-effect" tabindex="-1"><a class="header-anchor" href="#fundamental-hypotheses-coupling-effect" aria-hidden="true">#</a> Fundamental hypotheses: coupling effect</h2><h3 id="evolution-of-coupling-effect" tabindex="-1"><a class="header-anchor" href="#evolution-of-coupling-effect" aria-hidden="true">#</a> Evolution of coupling effect</h3><p>[Paper] Hints on Test Data Selection: Help for the Practicing Programmer</p><ul><li>The coupling effect: Test data that distinguishes all programs differing from a correct one by only simple errors is so sensitive that it also implicitly distinguishes more complex errors.</li><li>It&#39;s an empirical principle. No hope of proving the coupling effect.</li><li>They use a test suite that can kill all the 1-order mutant and see if this suite also work fro higher-order mutants</li><li>Impressive Result: They selected more than 22,000 higher-order mutants, and all are all killed.</li><li>（Conclusion: Hey! We find a phenomenon!)</li></ul><p>[Paper] Investigations of the Software Testing Coupling Effect</p><ul><li><p>More formal and sophisticated definitions. The author suspect the impressive result of the previous paper.</p></li><li><p>Definition of simple/complex fault: A simple fault is a fault that can be fixed by making a single change to a source statement. A complex fault is a fault that cannot be fixed by making a single change to a source statement.</p></li><li><p>Mutation Coupling Effect: Complex mutants are coupled to simple mutants in such a way that a test data set that detects all simple mutants in a program will detect a large percentage of the complex mutants.</p><ul><li>Complex mutants can represent the complex faults.</li><li>Complex faults are easier to detect than complex mutants.</li></ul></li><li><p>Operators:</p><ul><li>26 operators from another paper</li><li>Hint: Use <strong>ALL</strong> operators you can find, since we need to cover as many as possible &#39;complex faults&#39;.</li></ul></li><li><p>Result:</p><ul><li>Test data for 1-order mutants are also sucessful in 2-order and 3-order mutants, although still some higher-order mutants alive.</li><li>They can only say the two &#39;Effect&#39; are likely, but they didn&#39;t find evidence to go further.</li></ul></li><li><p>Conclusion:</p><ul><li>Mutation testing can only fucus on 1-order mutants and ignore n-order mutants.</li></ul></li></ul><h3 id="our-research" tabindex="-1"><a class="header-anchor" href="#our-research" aria-hidden="true">#</a> Our research</h3><p>One empirical research: our tool should allow to generate higher-order mutants (2-order and 3-order) and see if a sufficient test case for 1-order mutants can also kill most of the higher order mutants.</p><p>Impotant remarks</p><ul><li>Sufficient test case for 1-order mutants: at least 95% mutatation score</li><li>Use all the operators we find.</li></ul><h2 id="possible-benchmark-projects" tabindex="-1"><a class="header-anchor" href="#possible-benchmark-projects" aria-hidden="true">#</a> Possible benchmark projects</h2><p>From the papers I read I found the following benchmark proejcts they used. I will proof read them and figure out how they can be used in kernel tuner, creating our own benchmarks.</p>`,54),ke={href:"https://github.com/nvidia/cuda-samples",target:"_blank",rel:"noopener noreferrer"},be={href:"https://github.com/yuhc/gpu-parboil",target:"_blank",rel:"noopener noreferrer"},ve={href:"https://www.cs.virginia.edu/~skadron/Papers/rodinia_iiswc09.pdf",target:"_blank",rel:"noopener noreferrer"},ye={href:"https://github.com/sgrauerg/polybenchGpu",target:"_blank",rel:"noopener noreferrer"},_e=e("h2",{id:"plan-for-next-sprint",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#plan-for-next-sprint","aria-hidden":"true"},"#"),n(" Plan for next sprint")],-1),we=e("ul",null,[e("li",null,"[ ] Mutation testing: Continue dig into the mutation testing theories we find in week 3 and see what we can do in our research"),e("li",null,"[ ] Coverage: find out if we need coverage in our mutation testing research, and which can be the suitable one/ones"),e("li",null,"[ ] Benchmark project"),e("li",null,"[ ] Enrich the outline of the proposal based on what we find."),e("li",null,"[ ] Related work and some experiment we will conduct in this reasearch."),e("li",null,"[ ] GPU-verify")],-1);function xe(Ce,Ue){const t=i("ExternalLinkIcon");return l(),r("div",null,[c,e("ul",null,[u,e("li",null,[d,e("ol",null,[h,e("li",null,[m,g,e("ul",null,[e("li",null,[e("p",null,[n("Design/define "),e("a",f,[n("mutators"),a(t)]),n(" and patterns for CUDA/OpenCL")])]),k])]),b])]),v]),y,e("p",null,[n("*"),e("a",_,[n("Testing device functions"),a(t)]),n("? What is device functions? "),e("a",w,[n("Answer"),a(t)]),n(". How to test? Maybe need to figure out later.")]),x,e("ul",null,[C,e("li",null,[U,e("p",null,[n("​ The shared memory management is the main cause of data races and "),e("a",T,[n("bank conflicts"),a(t)]),n(".")]),P])]),I,e("ol",null,[e("li",null,[A,e("p",null,[n("I find a fix pattern in the beginer "),e("a",G,[n("tutorial"),a(t)]),n(": For cases where the number of elements in the arrays is not evenly divisible by the thread block size, the kernel code must check for out-of-bounds memory accesses.")])]),D]),M,S,q,L,O,R,e("p",null,[n("In this paper the author presents a mutation "),e("a",N,[n("testing tool"),a(t)]),n(" for OpenCL written in C++, using "),e("a",W,[n("only traditional mutators"),a(t)]),n(".")]),E,e("ol",null,[e("li",null,[K,e("ol",null,[j,e("li",null,[n("Todos: "),e("ul",null,[z,B,e("li",null,[n("[x] A "),e("a",H,[n("basic tutorial"),a(t)]),n(" of CUDA programming and "),e("a",F,[n("programming guide"),a(t)]),n(" from NVIDIA.")]),V])])])]),e("li",null,[Q,J,e("ul",null,[Z,e("li",null,[n("[x] Try the "),e("a",Y,[n("examples"),a(t)]),n(" and tutorials in kernel Tuner GitHub projects")]),X])]),$]),ee,e("ol",null,[e("li",null,[ne,e("ol",null,[e("li",null,[e("p",null,[n("NVIDIA's "),e("a",te,[n("document"),a(t)]),n(".")]),ae,e("p",null,[n("Kernel Tuner has already offer a "),e("a",se,[n("way"),a(t)]),n(" to perform Reference Comparison. In this research we will mainly focus on the unit test.")])]),oe])]),e("li",null,[ie,le,e("ol",null,[e("li",null,[e("a",re,[n("This paper"),a(t)]),n(" offer some mutation operators for OpenCL.")]),e("li",null,[e("a",pe,[n("This paper"),a(t)]),n(" we have already read in week 1 offer some mutation operators for CUDA.")]),e("li",null,[e("a",ce,[n("This docs"),a(t)]),n(" offer quite a lot of mutation operators for JavaScript, Java and C# that we can refer to.")])])]),ue,e("li",null,[de,e("ul",null,[e("li",null,[n("Kernel tuner offer "),e("a",he,[n("a way to measure performance"),a(t)]),n(" using metrics and customized objectives.")]),me,ge])])]),fe,e("ul",null,[e("li",null,[e("a",ke,[n("CUDA-Sample"),a(t)]),n(" from NVIDIA CUDA SDK")]),e("li",null,[e("a",be,[n("parboil"),a(t)]),n(" includes many benchmark projects, from University of Illinois")]),e("li",null,[e("a",ve,[n("Rodinia"),a(t)]),n(" benckmarks, from University of Virginia (kernel tuner has some)")]),e("li",null,[e("a",ye,[n("PolyBench/GPU"),a(t)]),n(", from University of Delaware")])]),_e,we])}const Ie=o(p,[["render",xe],["__file","mutation-testing-gpu.html.vue"]]);export{Ie as default};
